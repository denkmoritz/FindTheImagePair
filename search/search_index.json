{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Find The Image Pair","text":"<p>The goal of this project is to identify image pairs that share a similar visual appearance. For example, consider a street crossing. Street-view images are usually captured from a car, so the perspective is from the middle of the road. As a pedestrian, however, your view would be from the sidewalk. Although both perspectives show the same scene, they look quite different.</p> <p>This project focuses on finding pairs of images that represent the same view and have a similar heading. The aim is to build a dataset of such pairs. This dataset can then be used in studies where people are asked to rank images based on qualities such as beauty or to compare how different perspectives influence perception.</p> <p>The repository provides tools to simplify the process of creating this dataset. The data used comes from Mapillary.</p>"},{"location":"#whats-next","title":"What\u2019s next","text":"<ul> <li>Getting Started </li> <li>Guide</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>There are two ways to run Find The Image Pair. You can either pull the project via Docker or if you want to do modifications to the code you can also run it locally.</p>"},{"location":"getting-started/installation/#via-docker","title":"Via Docker","text":"<p>COMING SOON</p>"},{"location":"getting-started/installation/#local-installment","title":"Local Installment","text":"<p>To run it locally, run the following steps:</p> <pre><code>https://github.com/denkmoritz/FindTheImagePair.git\ncd FindTheImagePair\n</code></pre> <p>Add your Mapillary API key (not necessary):</p> <pre><code># in the image_finder dir\ntouch .env # MAPILLARY_TOKEN=&lt;YOUR_API_KEY&gt;\n</code></pre>"},{"location":"getting-started/installation/#set-up-the-backend","title":"Set up the Backend","text":"<pre><code>cd backend\n</code></pre>"},{"location":"getting-started/installation/#setup-images","title":"Setup Images","text":"<ol> <li>Download the <code>images.zip</code> file</li> <li>Place it in the <code>backend/</code> directory</li> <li>Unzip the file</li> </ol>"},{"location":"getting-started/installation/#docker-compose","title":"Docker Compose","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"getting-started/installation/#set-up-the-frontend","title":"Set up the Frontend","text":"<pre><code>cd frontend\nnpm install\n</code></pre> <p>To run the Frontend:</p> <pre><code>npm run dev\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":""},{"location":"getting-started/quickstart/#run-backend","title":"Run Backend","text":"<p>(Assumes that dir is <code>backend/</code>)</p> <p>First of all a csv-file with all possible tables has to generated. Simply run:</p> <pre><code>python city.py\n</code></pre> <p>To start the Backend:</p> <pre><code>uvicorn main:app --host 0.0.0.0 --reload\n</code></pre>"},{"location":"getting-started/quickstart/#run-frontend","title":"Run Frontend","text":"<p>(Assumes that dir is <code>frontend/</code>)</p> <pre><code>npm run dev\n</code></pre> <p>The frontend now listens to http://localhost:5173/ (use any browser to open it).</p>"},{"location":"guide/advanced/","title":"Advanced","text":"<p>This section explains how to generate your own tables for a city.</p> <p>Follow these steps:</p>"},{"location":"guide/advanced/#1-choose-a-city-create-the-initial-table","title":"1. Choose a city / create the initial table","text":"<p>Configure <code>initiate_table.py</code>:</p> <ul> <li>Choose the city name, table name, and WGS84/UTM CRS for the city</li> <li>Make sure the PostgreSQL credentials are correct</li> </ul> <p>Run the script:</p> <pre><code>python3 initiate_table.py\n</code></pre>"},{"location":"guide/advanced/#2-run-the-master-pipeline","title":"2. Run the master pipeline","text":"<pre><code>python3 master_pipeline.py\n</code></pre> <p>By default it uses the following example. However, if the city should be changed, simply all the variables can be easily altered.</p> <p>Example: <pre><code>CITY=\"Cape Town\" CITY_EPSG=\"32734\" BBOX_WEST=\"18.3\" BBOX_SOUTH=\"-34.1\" BBOX_EAST=\"18.5\" BBOX_NORTH=\"-33.8\" INNER=\"5\" OUTER=\"20\" MLY_SCORE=\"0.9\" python3 master_pipeline.py\n</code></pre></p>"},{"location":"guide/advanced/#3-test-different-thresholds","title":"3. Test different thresholds","text":"<p>By default, the original <code>filter_output.py</code> script has different thresholds for:</p> Option Short Description Default <code>--contrast-threshold</code> <code>-C</code> Minimum contrast score <code>0.35</code> <code>--tone-mapping-threshold</code> <code>-H</code> Minimum tone-mapping score <code>0.35</code> <code>--tone-mapping-floor</code> \u2014 Tone-mapping floor <code>0.8</code> <p>After testing, we only found that <code>-C</code> is useful. The modified script <code>filter_multiple.py</code> uses for <code>-C</code> five different values and the other two variables the default thresholds.</p> <pre><code>python3 filter_multiple.py\n</code></pre>"},{"location":"guide/advanced/#4-decide-which-threshold-to-use","title":"4. Decide which threshold to use","text":"<p>There is no such a thing as the perfect threshold, that is why the user should decide on their own which one to use. Depending on the size of the desired table a smaller or larger threshold can be chosen. The script <code>compare.py</code> provides assistance.</p> <pre><code>python3 compare.py\n</code></pre>"},{"location":"guide/advanced/#5-new-table-delete-entries","title":"5. New table &amp; delete entries","text":"<p>After deciding for value, the final script can be used. It creates a new table from the original city table and keeps only the accepeted ids'. The new table is then called <code>city_cvalue</code>, e.g., <code>cape_town_035</code>.</p> <pre><code>python3 delete_entries.py -C 0.35 # example value\n</code></pre>"},{"location":"guide/configuration/","title":"Configuration","text":""},{"location":"guide/configuration/#1-add-api-key","title":"1. Add API Key","text":"<p>The token can be acquired under the offical mapillary developer website. After registration, add to key to a .env file:</p> <pre><code>touch .env\n</code></pre> <pre><code>MAPILLARY_TOKEN=&lt;TOKEN&gt;\n</code></pre>"},{"location":"guide/configuration/#2-download-the-docker-postgresql-table","title":"2. Download the docker PostgreSQL table","text":"<pre><code>docker pull moritzdenk/postgis-global-streetscapes:latest\n</code></pre> <pre><code>docker run -d \\\n  --name postgis-global-streetscapes \\\n  -p 25433:5432 \\\n  -e POSTGRES_PASSWORD=postgres \\\n  moritzdenk/postgis-global-streetscapes:latest\n</code></pre>"},{"location":"guide/dataset/","title":"Dataset","text":"<p>The data used in this project originate from the NUS Global Streetscapes dataset, which was published by the National University of Singapore. The dataset contains large-scale street-level imagery with a variety of labels. For this project, these labels were combined into a single large table that includes the complete set of images and metadata.</p> <p>Notebook</p> <p>For the code and steps used to build the table, see Initial Table Generation.</p>"},{"location":"guide/dataset/#steps-for-preparing-the-subsets","title":"Steps for preparing the subsets","text":""},{"location":"guide/dataset/#step-1","title":"Step 1","text":"<p>The following table shows how the total count of the chosen cities in the study change when using the <code>mly_quality_score</code>. This is quite important to notice, hence, the amount changes quite drastically when using a different thresholds. </p> <p>Note</p> <p>This approach is useful when computation time needs to be reduced and fewer pairs are expected.</p> City Total 50 % 60 % 70 % 80 % 90 % Berlin 198184 61606 59728 56517 51531 41767 Washington 197080 76859 70041 60128 44313 24971 Sydney 69227 63944 61771 57759 52210 41051 Cape Town 12639 11135 10136 8764 6708 4068 Taipei 198538 171232 161789 146595 122037 84761 Sao Paulo 197964 129330 108546 78852 46080 19913 <p>Since testing showed, that the <code>heading</code> variable is not a 100 % reliable, mapillary's <code>computed heading</code> was used as well to determine if there is an offset greater than 10 degrees. The following reduction of the table looks the following:</p> City Total 50 % 60 % 70 % 80 % 90 % Berlin 45650 13035 12476 11629 10287 8018 Washington 132952 51160 47204 41233 30535 16644 Sydney 15304 12849 12164 11228 9924 7573 Cape Town 9511 8276 7416 6303 4618 2594 Taipei 26417 19805 18686 16351 12673 8040 Sao Paulo 124394 79460 66792 48881 27401 11107 <p>As a result, the difference between the headings and the score were being used to reduce the size of the images.</p>"},{"location":"guide/dataset/#step-2","title":"Step 2","text":"<p>Since the initial amount of images was still too big, we decided to use the tool by Danish et al. (2024) which can be found on GitHub. For our usage, the tools were slightly modified. How to use it, is explained in the See the Advanced guide. </p>"},{"location":"guide/dataset/#final-dataset","title":"Final dataset","text":"<p>The result of these steps is a cleaned and filtered subset of the Global Streetscapes dataset. In the Berlin example, this produced a smaller but higher quality collection of images that can be used for tasks such as pair identification and perception studies.</p>"},{"location":"guide/overview/","title":"Overview","text":"<p>See the Dataset guide to get an understanding on how the dataset was generated. In the section Configuration explains how to set up the Advanced guide. The guides purpose is that anybody can recreate tables for different cities or for different variables as they want to do it.</p>"},{"location":"notebooks/initial_table/","title":"Create Dataset","text":"In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import hf_hub_download\nimport pandas as pd\n</pre> from huggingface_hub import hf_hub_download import pandas as pd <p>The following files where chosen since they were considered as quite useful (really subjective though).</p> In\u00a0[\u00a0]: Copied! <pre># Define dataset ID and target filenames\ndataset_id = \"NUS-UAL/global-streetscapes\"\nfiles = [\n    \"metadata_common_attributes.csv\",\n    \"metadata_mly1.csv\",\n    \"metadata_mly2.csv\",\n    \"osm.csv\",\n    \"perception.csv\",\n    \"places365.csv\",\n    \"season.csv\",\n    \"simplemaps.csv\",\n    \"contextual.csv\",\n]\n\n# Download each file from the \"data/\" folder in the dataset repo\nlocal_paths = []\nfor fname in files:\n    path = hf_hub_download(\n        repo_id=dataset_id,\n        repo_type=\"dataset\",\n        filename=f\"data/{fname}\",\n        local_dir=\"data/\",\n        local_dir_use_symlinks=False,\n    )\n    local_paths.append(path)\n    print(f\"Downloaded {fname} -&gt; {path}\")\n\n# Optional: load one of the CSVs into pandas\nimport pandas as pd\n\ndf = pd.read_csv(local_paths[0])\ndf.head()\n</pre> # Define dataset ID and target filenames dataset_id = \"NUS-UAL/global-streetscapes\" files = [     \"metadata_common_attributes.csv\",     \"metadata_mly1.csv\",     \"metadata_mly2.csv\",     \"osm.csv\",     \"perception.csv\",     \"places365.csv\",     \"season.csv\",     \"simplemaps.csv\",     \"contextual.csv\", ]  # Download each file from the \"data/\" folder in the dataset repo local_paths = [] for fname in files:     path = hf_hub_download(         repo_id=dataset_id,         repo_type=\"dataset\",         filename=f\"data/{fname}\",         local_dir=\"data/\",         local_dir_use_symlinks=False,     )     local_paths.append(path)     print(f\"Downloaded {fname} -&gt; {path}\")  # Optional: load one of the CSVs into pandas import pandas as pd  df = pd.read_csv(local_paths[0]) df.head() In\u00a0[\u00a0]: Copied! <pre># Base CSV: Filter by source = 'Mapillary'\nmetadata_common = pd.read_csv(\"data/metadata_common_attributes.csv\", usecols=['uuid', 'lat', 'lon', 'heading', 'orig_id', 'source'])\nmetadata_common = metadata_common[metadata_common['source'] == 'Mapillary']\n\n# Function to safely merge another CSV by uuid\ndef merge_csv(df_base, file_path, usecols):\n    df_add = pd.read_csv(file_path, usecols=usecols)\n    return df_base.merge(df_add, on='uuid', how='left')\n\n# Sequentially merge all additional files\nmetadata_common = merge_csv(metadata_common, \"data/metadata_mly1.csv\", ['uuid', 'mly_quality_score'])\nmetadata_common = merge_csv(metadata_common, \"data/metadata_mly2.csv\", ['uuid', 'mly_computed_compass_angle'])\nmetadata_common = merge_csv(metadata_common, \"data/osm.csv\", ['uuid', 'type_highway'])\nmetadata_common = merge_csv(metadata_common, \"data/perception.csv\", None)  # None = all columns\nmetadata_common = merge_csv(metadata_common, \"data/places365.csv\", ['uuid', 'place'])\nmetadata_common = merge_csv(metadata_common, \"data/season.csv\", ['uuid', 'season'])\nmetadata_common = merge_csv(metadata_common, \"data/simplemaps.csv\", ['uuid', 'city_ascii', 'city_id', 'iso3', 'admin_name'])\nmetadata_common = merge_csv(metadata_common, \"data/contextual.csv\", ['uuid', 'platform', 'view_direction', 'quality'])\n\n# Save final result\nmetadata_common.to_csv(\"data/joined_metadata.csv\", index=False)\n\nprint(\"Final dataset shape:\", metadata_common.shape)\nprint(metadata_common.head())\n</pre> # Base CSV: Filter by source = 'Mapillary' metadata_common = pd.read_csv(\"data/metadata_common_attributes.csv\", usecols=['uuid', 'lat', 'lon', 'heading', 'orig_id', 'source']) metadata_common = metadata_common[metadata_common['source'] == 'Mapillary']  # Function to safely merge another CSV by uuid def merge_csv(df_base, file_path, usecols):     df_add = pd.read_csv(file_path, usecols=usecols)     return df_base.merge(df_add, on='uuid', how='left')  # Sequentially merge all additional files metadata_common = merge_csv(metadata_common, \"data/metadata_mly1.csv\", ['uuid', 'mly_quality_score']) metadata_common = merge_csv(metadata_common, \"data/metadata_mly2.csv\", ['uuid', 'mly_computed_compass_angle']) metadata_common = merge_csv(metadata_common, \"data/osm.csv\", ['uuid', 'type_highway']) metadata_common = merge_csv(metadata_common, \"data/perception.csv\", None)  # None = all columns metadata_common = merge_csv(metadata_common, \"data/places365.csv\", ['uuid', 'place']) metadata_common = merge_csv(metadata_common, \"data/season.csv\", ['uuid', 'season']) metadata_common = merge_csv(metadata_common, \"data/simplemaps.csv\", ['uuid', 'city_ascii', 'city_id', 'iso3', 'admin_name']) metadata_common = merge_csv(metadata_common, \"data/contextual.csv\", ['uuid', 'platform', 'view_direction', 'quality'])  # Save final result metadata_common.to_csv(\"data/joined_metadata.csv\", index=False)  print(\"Final dataset shape:\", metadata_common.shape) print(metadata_common.head()) In\u00a0[\u00a0]: Copied! <pre># Read file\ndata = pd.read_csv(\"data/joined_metadata.csv\")\nprint(data.head())\n</pre> # Read file data = pd.read_csv(\"data/joined_metadata.csv\") print(data.head()) In\u00a0[\u00a0]: Copied! <pre># Create gdf\ngdf = gpd.GeoDataFrame(\n    data, geometry=gpd.points_from_xy(data.lon, data.lat), crs=\"EPSG:4326\"\n)\n</pre> # Create gdf gdf = gpd.GeoDataFrame(     data, geometry=gpd.points_from_xy(data.lon, data.lat), crs=\"EPSG:4326\" ) In\u00a0[\u00a0]: Copied! <pre>from getpass import getpass\nfrom urllib.parse import quote_plus\n# Database connection info\n# Change credentials if necessary\n\nhost = \"localhost\"\ndatabase = \"gis\"\nuser = \"moritz\"\nport = \"25432\"\npassword = getpass(\"Enter your password: \")\n\n# URL-encode the password to handle special characters\nencoded_password = quote_plus(password)\n\n# Add the port in the connection string\nconnection_string = f\"postgresql://{user}:{encoded_password}@{host}:{port}/{database}\"\n\nfrom sqlalchemy import create_engine\n\nengine = create_engine(connection_string)\n\n# If you want to use %sql magic in Jupyter\n%reload_ext sql\n%sql $connection_string\n%config SqlMagic.style = 'DEFAULT'\n</pre> from getpass import getpass from urllib.parse import quote_plus # Database connection info # Change credentials if necessary  host = \"localhost\" database = \"gis\" user = \"moritz\" port = \"25432\" password = getpass(\"Enter your password: \")  # URL-encode the password to handle special characters encoded_password = quote_plus(password)  # Add the port in the connection string connection_string = f\"postgresql://{user}:{encoded_password}@{host}:{port}/{database}\"  from sqlalchemy import create_engine  engine = create_engine(connection_string)  # If you want to use %sql magic in Jupyter %reload_ext sql %sql $connection_string %config SqlMagic.style = 'DEFAULT' In\u00a0[\u00a0]: Copied! <pre># Create table\ngdf.to_postgis('global_streetscapes', engine, if_exists='replace', index=False)\n</pre> # Create table gdf.to_postgis('global_streetscapes', engine, if_exists='replace', index=False) In\u00a0[\u00a0]: Copied! <pre># Create spatial index\n%%sql\nDROP INDEX IF EXISTS idx_global_streetscapes_spgist_geometry;\nCREATE INDEX idx_global_streetscapes_spgist_geometry ON global_streetscapes USING SPGIST (geometry)\n</pre> # Create spatial index %%sql DROP INDEX IF EXISTS idx_global_streetscapes_spgist_geometry; CREATE INDEX idx_global_streetscapes_spgist_geometry ON global_streetscapes USING SPGIST (geometry) In\u00a0[\u00a0]: Copied! <pre>%%sql\nSELECT * FROM global_streetscapes LIMIT 1;\n</pre> %%sql SELECT * FROM global_streetscapes LIMIT 1;"},{"location":"notebooks/initial_table/#create-dataset","title":"Create Dataset\u00b6","text":""},{"location":"notebooks/initial_table/#download-global-streetscapes-metadata-from-nus-ualglobal-streetscapes","title":"Download Global Streetscapes Metadata from NUS-UAL/global-streetscapes\u00b6","text":""},{"location":"notebooks/initial_table/#combine-metadata-in-one-file","title":"Combine metadata in one file\u00b6","text":""},{"location":"notebooks/initial_table/#build-gpd-dataframe","title":"Build gpd-Dataframe\u00b6","text":""},{"location":"notebooks/initial_table/#connect-to-postgresql","title":"Connect to PostgreSQL\u00b6","text":""}]}